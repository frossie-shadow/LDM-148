:tocdepth: 1

.. sectnum::

.. _overview:

########
Overview
########

DRAFT

Still to be done:
 - fill out more text
 - internal links
 - synchronize with OpsCon documents


The LSST Data Management System (DMS) is a set of services employing a variety of software components running on computational and networking infrastructure that combine to deliver science data products to the observatory's users and support observatory operations.
The DMS is constructed by the DM subsystem in the NSF MREFC project; in the Operations era, it is operated by a combination of the LSST Data Facility, Science Operations, and Observatory Operations departments.

The data products to be delivered are defined and described in the Data Products Definition Document (LSE-163).
These are divided into three major categories.

One category of data products is generated on a nightly or daily cadence and comprises raw, calibrated, and difference images as well as alerts of transient, moving, and variable objects detected from the images, published within 60 seconds, and recorded in searchable catalogs.
These data products can be considered "online", as they are driven primarily by the observing cadence of the observatory.
This category has historically been referred to as "Level 1".

A second category of data products is generated on an annual cadence and represents a complete reprocessing of the set of images taken to date to generate astronomical catalogs containing measurements and characterization of tens of billions of stars and galaxies with high and uniform astrometric and photometric accuracy.
As part of this reprocessing, all of the first category of data products is regenerated, often using more accurate algorithms.
This category also includes other data products such as calibration products and templates that are generated in an "offline" mode, not directly tied to the observing cadence.
This category has historically been referred to as "Level 2", including the regenerated data products from the first category.

The third category of data products is not generated by the LSST DMS but is instead generated, created, or imported by science users.
These products derive value from their close association with or derivation from other LSST data products.
The DMS is responsible for providing facilities, services, and software for their generation and storage.
This category has historically been referred to as "Level 3".

Science data products are delivered through Data Access Centers (DACs), plus streams of near-realtime alerts and telescope pointing predictions.
Each LSST data product has associated metadata providing provenance and quality metrics and tracing it to relevant calibration information in the archive.
The DACs are composed of modest but significant computational, storage, networking, and other resources intended for use as a flexible, multi-tenant environment for professional astronomers with LSST data rights to retrieve, manipulate, and annotate LSST data products in order to perform scientific discovery and inquiry.

The services that make up the DMS are in turn made up of software and underlying service components, instantiated in a particular configuration in a particular computing environment to perform a particular function.
Some software components are specific to a service; others are general-purpose and reused across multiple services.
Many services have only one instance in the production system; others have several, and all have additional instances in the development and integration environments for testing purposes.

The DMS services can be considered to consist of four tiers of software components.
The top tier is the LSST Science Platform, which is deployed in the DACs and other computational environments to provide a user interface and analysis environment for science users and LSST staff.
The detailed design of this tier is given in SUIT Design, LDM-131.
The next tier is composed of science "applications" software that generates data products.
This software is used to build "payloads", sequences of pipelines, that perform particular data analysis and product generation tasks.
It is also used by science users and staff to analyze the data products.
The detailed design of the components in this tier is given in Data Management Science Pipelines Design, LDM-151.
A lower tier is "middleware" software components and services that execute the science application payloads and isolate them from their environment, including changes to underlying technologies.
These components also provide data access for science users and staff.
The detailed design of the components in this tier is given in Data Management Middleware Design, LDM-152.
The bottom tier is "infrastructure": hardware, networking, and low-level software and services that provide a computing environment.
The detailed design of components in this tier is given in Data Management Services & Infrastructure, LDM-129, and Network Design, LSE-78.

The DMS computing environments reside in four main physical locations: the Summit Site including the main Observatory and Auxiliary Telescope buildings on Cerro Pachon, Chile; the Base Facility data center located at the Base Site in La Serena, Chile; the Archive Facility data center at the National Center for Supercomputing Applications (NCSA) in Urbana, Illinois, USA; and the Satellite Computing Facility at CC-IN2P3 in Lyon, France.
These are linked by high-speed networks to allow rapid data movement.
The Base and Archive Facilities include production computational environments (the Base Enclave and NCSA Enclave, respectively) and also the US and Chilean Data Access Centers.
In addition, a Commissioning Cluster computational environment also resides at the Base Facility.

The DMS service instances in the NCSA Enclave can be broken down into three main functional domains: a near-realtime online domain (L1) closely linked to the rest of the Observatory; an offline Level 2 domain (L2) organized primarily around the annual Data Release Production; and an analysis and developer support domain (ADS) encompassing environments that operations staff use for science validation, software development, system integration, and system testing.
In addition, an underlying infrastructure domain (Infra) hosts services supporting all of the other domains, including a common Data Backbone that provides data transport and archiving and that is the primary connection between all of the domains.
These domains are distinguished by having different users, operations timescales, interfaces, and often components.

The service instances that make up the DMS include (with the computational environment or domains they are in noted):
 - Image and EFD Archiving (Base)
 - Prompt Processing Ingest (Base)
 - Observatory Control System (OCS) Driven Batch Control (Base)
 - Telemetry Gateway (Base)
 - Prompt Processing (L1)
 - OCS Driven Batch Processing (L1)
 - Offline Processing (L1)
 - Alert Distribution (L1)
 - Alert Filtering (L1)
 - Level 1 Quality Control (QC) (L1)
 - Template and Calibration Products Production Execution (L2)
 - Data Release Production Execution (L2)
 - Data Release Production Satellite Processing (Satellite Computing)
 - Level 2 QC (L2)
 - LSST Science Platform Commissioning Cluster instance (Commissioning Cluster)
 - LSST Science Platform Data Access Center instances (DACs)
 - Bulk Data Distribution (DAC)
 - LSST Science Platform Science Validation instance (ADS)
 - Developer Services (ADS)
 - Integration and Test (ADS)
 - Data Backbone (Infra)
 - Management/Monitoring (Infra)
 - Provisioning/Deployment (Infra)
 - Workload/Workflow (Infra)
 - HTCondor Batch Processing (Infra)
 - Identity Management (Infra)

The relationships between these services, their deployment environments, functional domains, and science application "payloads" can be visualized in this diagram:

.. figure:: /_static/DMS_Deployment.png

    Data Management System Deployment

The common infrastructure services are illustrated in this diagram:

.. figure:: /_static/DMS_Common_Services.png

    Data Management System Common Infrastructure Services



The science application software for the Alert Production, daytime processing, Data Release Production, and calibration processing is built out of a set of frameworks that accept plugins.
In turn, those frameworks build on middleware that provides portability and scalability.

Key applications software components include:
 - Low-level astronomical software primitives and data structures (``afw``)
 - Image processing and measurement framework with core algorithms (``ip_*``, ``meas_*``)
 - Additional image processing and measurement algorithms (``meas_extensions_*``)
 - High-level algorithms and driver scripts that define pipelines (``pipe_tasks``, ``pipe_drivers``)
 - Camera-specific customizations (``obs_*``)

.. figure:: /_static/DM_Application_Software_Arch.png

    Data Management Science Pipelines Software "Stack"


Key middleware components include:
 - Data access client (Data Butler) (``daf_persistence``)
 - Parallel distributed database (``qserv``)
 - Task framework (``pex_*``, ``log``, ``pipe_base``, ``ctrl_pool``)
 - Workflow and orchestration for production control (``ctrl_*``)

Infrastructure components include:
 - Other databases (typically relational)
 - Filesystems
 - Authentication and authorization (identity management)
 - Provisioning and resource management
 - Monitoring

.. figure:: /_static/DM_Middleware_and_Infrastructure.png

    Data Management Middleware and Infrastructure


.. _base-enclave:

############
Base Enclave
############

Services located in this enclave are located at the Base solely because they must interact with the OCS or the Camera Data System (also known as the Camera DAQ) or both.
In several cases, services located here interact closely with corresponding services in the NCSA Enclave's Level 1 Domain, to the point where the Base service cannot function if the NCSA service is not operational.
This reliance has been taken into account in the fault tolerance strategies used.

The primary goals of the services in this enclave are to transfer data to appropriate locations, either to NCSA, from NCSA, or to the Data Backbone.

The services in this enclave and their partners in the NCSA Enclave Level 1 Domain need to run rapidly and reliably.
They run at times (outside office hours) and with latencies that are not amenable to a human-in-the-loop design.
Instead, they are designed to execute autonomously, often under the control of the OCS, with human oversight, monitoring, and control only at the highest level.


.. _base-enclave-services:

Service Descriptions
====================

Detailed concepts of operations for each service can be found in "Concept of Operations for the LSST Production Services" (LDM-230).


Image and EFD Archiving
-----------------------

This component is composed of several Image Archiving service and Catch-Up Image Archiving instances: one pair each for the LSSTCam, the ComCam, and the Auxiliary Telescope Spectrograph, all of which may be operated simultaneously.
These capture raw images taken by each camera, including the wavefront sensors and the guide sensors of the LSSTCam or ComCam when so configured, retrieving them from their respective Camera Data System instances.
They also capture specific sets of metadata associated with the images, including telemetry values and event timings, from the OCS publish/subscribe middleware and/or from the EFD.
The image pixels and metadata are then permanently archived in the Data Backbone.
The catch-up versions archive into the Data Backbone any raw images and metadata that were missed by the primary archiving services due to network or other outage, retrieving them from the flash storage in the Camera Data System instances and the EFD.

This component also includes an EFD Transformation service that extracts all information (including telemetry, events, configurations, and commands) from the EFD and its large file annex, transforms it into a form more suitable for querying by image timestamp, and loads it into the permanently archived "Transformed EFD" database in the Data Backbone.

.. _prompt-processing-ingest:

Prompt Processing Ingest
------------------------

This component is composed of two instances that capture crosstalk-corrected images from the LSSTCam and ComCam Camera Data Systems along with selected metadata from the OCS and/or EFD and transfer them to the Prompt Processing service in the NCSA Enclave Level 1 Domain.

There is no Prompt Processing Ingest instance for the auxiliary telescope spectrograph.

.. _ocs-driven-batch-control:

OCS Driven Batch Control
------------------------

This service receives commands from the OCS and invokes the OCS Driven Batch Processing service in the NCSA Enclave Level 1 Domain to execute corresponding science payloads.
It is used for modest-latency analysis of images during Commissioning and for processing daily calibration images in normal observing operations.
A summary status for the processing performed is returned to the OCS for each command, following the normal OCS commanding protocol.

Telemetry Gateway
-----------------

This service obtains information from the NCSA Enclave Level 1 Domain, particularly status and quality metrics from Prompt Processing of images and the Level 1 Quality Control service, and transmits it to the OCS as specified in the Data Management-OCS Software Communication Interface (LSE-72).
Note that more detailed information on the status and performance of DMS services will also be available to Observatory operators through remote displays originated from the Management/Monitoring infrastructure services in all DMS computational environments.

.. _base-interfaces:

Interfaces
==========

OCS to all Base Enclave services: these interface through the SAL library provided by the OCS subsystem.

Archiver and Catch-Up Archiver to Data Backbone: files are copied to Data Backbone storage via a file transfer mechanism, and their information and metadata are registered with Data Backbone management dataabases.
The Data Butler is not used for this low-level, non-science-payload interface.

EFD to EFD Transformer: this interface is via connection to the MySQL databases that make up the EFD as well as file transfer from the EFD's Large File Annex.

EFD Transformer to Data Backbone: Transformed EFD entries are inserted into the "Transformed EFD" database resident within the Data Backbone.

Camera Data System to Archiver, Catch-Up Archiver, Prompt Processing Ingest: these interface through the custom library provided by the Camera Data System.

Prompt Processing Ingest to Prompt Processing: BBFTP is used to transfer files over the international network from the ingest service to the processing service.

OCS Driven Batch Control to OCS Driven Batch Processing: HTCondor is used to transfer execution instructions over the international network from the control service to the processing service and return status and result information.

Telemetry Gateway from NCSA Enclave Level 1 Domain services: RabbitMQ is used to transfer status and quality metrics to the gateway over the international network.



.. _level-1-domain:

###########################
NCSA Enclave Level 1 Domain
###########################

This domain is responsible for the compute-intensive processing for all near-realtime operations and other operations closely tied with the Observatory.
Its primary goals are to process images and metadata from the Observatory into "online" science data products and publish them to the DACs, alert subscribers, and back to the OCS.

The Prompt Processing, OCS Driven Batch Processing, and Offline Processing services support execution of science payloads in three different ways.
Prompt Processing is tightly integrated with the observing cadence and is intended to function in near-realtime with strict result deadlines.
OCS Driven Batch Processing is invoked by the OCS but has more modest latency requirements.
Offline Processing is not invoked by the OCS but operates under DMS control, typically during the daytime.

The Alert Distribution and Alert Filtering services receive batches of alerts resulting from Prompt Processing of each scienc visit; they then provide alert streams to community alert brokers and LSST data rights holders, respectively.

The Level 1 Quality Control service monitors the "online" science data products, including alerts, notifying operators if any anomalies are found.

Like the services in the Base Enclave, these services need to run rapidly and reliably and so are designed to execute autonomously.


.. _level-1-domain-services:

Service Descriptions
====================

Detailed concepts of operations for each service can be found in "Concept of Operations for the LSST Production Services" (LDM-230).


.. _prompt-processing:

Prompt Processing
-----------------

This service receives crosstalk-corrected images and metadata from the Prompt Processing Ingest service at the Base and executes the Alert Production science payload on them, generating "online" data products that are stored in the Data Backbone.
The Alert Production payload then sends alerts to the Alert Distribution service.

The Prompt Processing service has calibration (including Collimated Beam Projector images), science, and deep drilling modes.
In calibration mode, it executes a Raw Calibration Validation payload that provides rapid feedback of raw calibration image quality.
In normal science mode, two consecutive exposures are grouped and processed as a single visit.
Definitions of exposure groupings to be processed as visits in deep drilling and other modes are TBD.
The service is required to deliver Alerts within 60 seconds of the final camera readout of a standard science visit with 98% reliability.

There is no Prompt Processing service instance for the Auxiliary Telescope Spectrograph.

.. _ocs-driven-batch:

OCS Driven Batch Processing
---------------------------

This service executes science payloads in response to commands from the OCS Driven Batch Control service at the Base and thus indirectly from the Observatory Control System.
It is used for modest-latency analysis of images during Commissioning and for processing daily calibration images in normal observing operations.
Images and metadata are taken from the Data Backbone, and results are provided back to the Data Backbone; there is no direct connection from this service to the Camera Data System.
This obviously bounds the minimum latency from image acquisition to processing start by the latency of the Archiving service and Data Backbone transfer.
A summary status for the processing performed is sent to the OCS Driven Batch Control service to be returned to the OCS.


.. _offline-processing:

Offline Processing
------------------

This service executes science payloads to ensure that all Level 1 data products are generated within 24 hours.
In particular, this service executes the daytime Moving Object Processing System payload.
It also may execute a variant of the Alert Production payload if the Prompt Processing service encounters difficulties.
Images and metadata are taken from the Data Backbone, and results are provided back to the Data Backbone.


.. _level-1-qc:

Level 1 Quality Control
-----------------------

This service collects information on Level 1 science and calibration payload execution, post-processes the science data products from the Data Backbone to generate additional measurements, and monitors the measurement values against defined thresholds, providing an automated quality control capability for potentially detecting issues with the environment, telescope, camera, data acquisition, or data processing.
Alarms stemming from threshold crossings are delivered to Observatory operators and to LSST Data Facility Production Scientists for verification, analysis, and resolution.


.. _alert-broker-feed:

Alert Distribution
------------------

This service obtains alerts generated by the Alert Production science payload and distributes them to community alert brokers and to the Alert Filtering service.


.. _alert-filtering:

Alert Filtering
---------------

This service obtains an alert feed from the Alert Broker Feed service and allows individual LSST data rights holders to execute limited filters against it, producing filtered feeds that are then distributed to the individuals.



.. _level-1-domain-interfaces:

Interfaces
==========

Prompt Processing to Alert Distribution and Alert Filtering: these interface through a reliable transport system.

Prompt Processing to Offline Processing: in the event that Prompt Processing runs over its allotted time window, processing can be cancelled and the failure recorded, after which Offline Processing will redo the processing at a later time.
Note that it may be possible, if sufficient computational resources have been provisioned, for the Prompt Processing to be allowed to continue to run, with spare capacity used to maintain latency for future visits.
In that case, there would effectively be an infinite time window.

Science Payloads to Data Backbone: payloads use the Data Butler as a client to access files and catalog databases within the Data Backbone.


.. _level-2-domain:

###########################
NCSA Enclave Level 2 Domain
###########################

This domain is responsible for all longer-period data processing operations, including the largest and most complex payloads supported by the DMS: the annual Data Release Production (DRP) and periodic Calibration Products Productions (CPPs).
Note that CPPs will execute even while the annual DRP is executing, hence the need for a separate service.
The Level 2 Quality Control Service monitors the science data products, notifying operators if any anomalies are found.

The services in this domain need to run efficiently and reliably over long periods of time, spanning weeks or months.
They need to execute millions or billions of tasks when their input data becomes available while tracking the status of each and preserving its output.
They are designed to execute autonomously with human oversight, monitoring, and control primarily at the highest level, although provisions are made for manual intervention if absolutely necessary.

This domain does not have direct users (besides the operators of its services); the services within it obtain inputs from the Data Backbone and place their outputs into the Data Backbone.


.. _level-2-services:

Service Descriptions
====================

.. _template-cpp-execution:

Template and Calibration Products Production Execution
------------------------------------------------------

This service executes various CPP science payloads at intervals to generate Master Calibration Images and populate the Calibration Database with information derived from analysis of raw calibration images from the Data Backbone and information in the Transformed EFD.
This includes the computation of crosstalk correction matrices.
Although not a calibration product, the templates used by Alert Production are also generated by this service, based on raw science images from the Data Backbone.
Additional information such as external catalogs are also taken from the Data Backbone.
The intervals at which this service executes will depend on the stability of Observatory systems but are expected to include at least monthly and annual executions.
The annual execution is a prerequisite for the subsequent execution of the Data Release Production.
The service involves human scientist/operator input to determine initial configurations of the payload, to monitor and analyze the results, and possibly to provide additional configuration information during execution.


.. _drp-execution:

Data Release Production Execution
---------------------------------

This service executes the DRP science payload annually to generate all Level 2 data products after the annual CPP is executed.
A small-scale (about 10% of the sky) mini-production is executed first to ensure readiness, followed by the full production.
Raw science images are taken from the Data Backbone along with Master Calibration Images and information from the Transformed EFD.
Additional information such as external catalogs may also be taken from the Data Backbone.
Computing is performed in conjunction with the DRP Satellite Processing service at CC-IN2P3, which will have capacity for half of the DRP processing.
Output data products from both the mini-production and the main production are loaded into the Data Backbone, including both images and catalogs.
From there, they are analyzed by LSST staff scientists and selected external scientists using the Science Validation instance of the LSST Science Platform to ensure quality and readiness for release.
The to-be-released data products are loaded into the Data Access Center services, and access is then enabled on the release date.
The service involves human scientist/operator/programmer input to determine initial configurations of the payload, to monitor and analyze results, and, when absolutely necessary, to make "hot fixes" during execution that maintain adequate consistency of the resulting data products.


.. _level-2-qc:

Level 2 Quality Control
-----------------------

This collects information on Level 2 science payload execution, post-processes the science data products from the Data Backbone to generate additional measurements, and monitors the measurement values against defined thresholds, providing an automated quality control capability for potentially detecting issues with the data processing but also the environment, telescope, camera, or data acquisition.
Alarms stemming from threshold crossings are delivered to LSST Data Facility Production Scientists for verification, analysis, and resolution.


.. _level-2-interfaces:

Interfaces
==========

Calibration Products Production Execution and Data Release Production Execution to Data Backbone: for large-scale productions, a workflow system is expected to stage files and selected database entries from the Data Backbone to local storage for access by the science payloads via the Data Butler.
Similarly, the staging system will ingest output images and catalogs into the Data Backbone.

.. _satellite-computing-enclave:

###########################
Satellite Computing Enclave
###########################

.. _satellite-computing-service:

Service Description
===================

.. _drp-satellite-processing:

Data Release Production Satellite Processing
--------------------------------------------

This service controls the processing of jobs on the CC-IN2P3 satellite computing facilities under the overall workload and workflow management of the Data Release Production Execution service at NCSA.


.. _dac-enclave:

###########################
Data Access Center Enclaves
###########################

There are two Data Access Centers, one in the US at NCSA and one in Chile at the Base.
These DACs are responsible for all science-user-facing services, primarily instances of the LSST Science Platform (LSP).
The LSP is the preferred analytic interface to LSST data products in the DAC.
It provides computation and data access on both interactive and asynchronous timescales.
The US DAC also includes a service for distributing bulk data on daily and annual (Data Release) timescales to partner institutions, collaborations, and LSST Education and Public Outreach (EPO).

The services in this domain must support multiple users simultaneously and securely.
The LSP must be responsive to science user needs; updates are likely to occur at a different cadence from the other domains as a result.
The LSP must operate reliably enough that scientific work is not impeded.


.. _dac-services:

Service Descriptions
====================

.. _bulk-data-distribution:

Bulk Data Distribution
----------------------

This service is used to transmit Level 1 and Level 2 data products to partners such as LSST Education and Public Outreach, the UK LSST project, and the Dark Energy Science Collaboration.
It extracts data products from the Data Backbone and transmits them over high bandwidth connections to designated, pre-subscribed partners.

.. _science-platform-dac:

LSST Science Platform DAC instances
-----------------------------------

This service provides an exploratory analysis environment for science users.  It can be further broken down into three "Aspects" that it presents to end users, along with underlying "backend services" that users can take advantage of, as illustrated here:

.. figure:: /_static/LSST_Science_Platform.png

    LSST Science Platform

The "Portal" Aspect provides a pre-specified yet flexible discovery, query, and viewing tool.
The "JupyterLab" Aspect provides a fully flexible ("notebook") environment incorporating rendering of images, catalogs, and plots and providing for execution of LSST-provided and custom algorithms.
The "Web API" Aspect provides a language-independent, VO-compliant Web Services data access API with extensions for LSST capabilities and volumes.
Access is provided via all three Aspects to all data products, including images, catalogs, and metadata.
The Web API Aspect regenerates "virtual" data products on demand when required.

The backend services provide general-purpose user computation, including batch job submission; user file storage modeled on VOSpace; and user database storage modeled on SkyServer MyDB.
Data may be shared with individual users, with groups, or with all DAC users (data rights holders).
Resource management of the backend services is based on a small "birthright" quota with additional resources allocated by a committee.

All usage of any LSST Science Platform instance requires authentication to ensure availability only to LSST data rights holders or LSST operations staff.


.. _dac-interfaces:

Interfaces
==========

[...]


.. _ads-domain:

##################################################
NCSA Enclave Analysis and Developer Support Domain
##################################################

This domain encompasses environments for analysts, developers, and integration and test.
Its users are the Observatory staff as they analyze raw data and processed data products to characterize them, develop new algorithms and systems, and test new versions of components and services before deployment.


.. _ads-services:

Service Descriptions
====================

.. _science-platform-science-validation:

LSST Science Platform Science Validation instance
-------------------------------------------------

This instance of the LSST Science Platform is customized to allow access to unreleased and intermediate data products from the Alert, Calibration Products, and Data Release Productions.
It is optimized for usage by scientists within the LSST Operations team, although selected external scientists can be granted access to assist with Science Validation.
Part of the optimization is to size and configure the three Aspects of the LSP appropriately; in particular, more JupyterLab usage and less portal usage is expected.

.. _developer-services:

Developer Services
------------------

Software version control service, packaging, build and unit test service, software release management, ticket tracking service, documentation services, etc.


Integration and Testing
-----------------------

Integration environments representing various deployment environments, deployment services, test datasets, test execution services, metric measurement and tracking services, etc.


.. _ads-interfaces:

Interfaces
==========

[...]


#####################
Commissioning Cluster
#####################

.. _science-platform-commissioning:

LSST Science Platform Commissioning instance
--------------------------------------------

This instance of the LSST Science Platform for Science Validation runs on the Commissioning Cluster at the Base Facility (but also has access to computational resources at the Archive) and accesses a Base endpoint for the Data Backbone.
This location at the Base lowers the latency of both access to Data Backbone-resident data (which does not have to wait for transfer over the international network) and, perhaps more importantly, for user interface operations for staff in Chile, which are served locally.
Note that the Commissioning Cluster does not have direct access to the Camera Data System; it relies on the Archiver service to obtain data.
The Commissioning Cluster will have direct access to the OCS's Base replica of the EFD (before transformation).



.. _infrastructure-domain:

#####################
Infrastructure Domain
#####################

This domain encompasses the underlying services and systems that form the computing environments in which the other domains are deployed and operate.
It interfaces with the other domains but has no direct users.


.. _infrastructure-services:

Service Descriptions
====================

.. _data-backbone:

Data Backbone
-------------

The Data Backbone is a key component that provides for data storage, transport, and replication, allowing data products to move between computational environments.
This service provides policy-based replication of files (in the Science Image Archive) and databases (in the Science Catalog Archive) across multiple physical locations, including the Base, Commissioning Cluster, NCSA, and DACs.
It manages caches of files at each endpoint as well as persistence to long-term archival storage (e.g. tape).
It provides a registration mechanism for new datasets and database entries and a retrieval mechanism compatible with the Data Butler.

The Qserv distributed database system for large-scale catalog data has instances within the Data Backbone in each DAC as well as in the NCSA Enclave.

The relationships between the Data Backbone components are illustrated in this diagram:

.. figure:: /_static/Data_Backbone.png

    Data Backbone


.. _management-monitoring:

Management/Monitoring
---------------------

These services provide management and monitoring at service and infrastructure systems levels for each enclave and domain.


.. _provisioning-deployment:

Provisioning/Deployment
-----------------------

These services provide compute, local-to-node storage, and local-to-LAN storage resources for all processing, including Prompt Processing, Batch Processing, and the Science Platforms.
They allow allocation of compute and storage resources as well as reproducible, controlled deployment of services onto those resources.

Some compute resources are reserved for particular uses, but others can be flexibly provisioned, up to a certain maximum quota, if needed to deal with surges in processing.

The priority order for processing is:
 - Prompt processing
 - Offline processing
 - OCS-controlled batch processing
 - LSP Commissioning Cluster processing
 - LSP Science Validation processing
 - LSP Data Access Center processing
 - Template and Calibration Products Production
 - Data Release Production

The Base Enclave's services are not highly dynamic or flexible, as they primarily provide interfacing to the OCS and Camera Data System.
The baseline provisioning for them is using vSphere; they will be deployed using Puppet.


.. _workload-workflow:

Workload/Workflow
-----------------

These services provide management of the execution of science payloads ranging from a single pipeline to a series of "campaigns", each consisting of multiple  pipelines.
They are able to handle massively distributed computing, executing jobs when their inputs become available and tracking their status and outputs.
They ensure that the data needed for a job is accessible to it and that outputs (including log files, if any) are preserved.
They can allocate work across multiple computing environments, in particular between NCSA and the Satellite Computing Facility at CC-IN2P3.


.. _batch-processing:

Batch Processing
----------------

This service provides execution of batch jobs with a variety of priorities from a variety of users in a variety of environments (e.g. OS and software configurations) on the underlying provisioned compute resources.
It will use containerization to handle heterogeneity of environments.
HTCondor is the baseline technology choice for this service.


.. _identity-management:

Identity Management
-------------------

This service provides authentication and authorization for all users of any DMS component, especially the LSST Science Platform instances.


.. _infrastructure-interfaces:

Interfaces
----------

[...]


.. _software-components:

###################
Software Components
###################

.. _science-payloads:

Science Payloads
================

Described in DM Applications Design Document (LDM-151).
Payloads are built from application software components.

.. _alert-production-payload:

Alert Production Payload
------------------------

Executes under control of the Prompt Processing service.
Generates all Level 1 science data products including Alerts (with the exception of Solar System object orbits) and loads them into the Data Backbone and Level 1 Database.
Transmits Alerts to Alert Distribution service.
Generates image quality feedback to the OCS and observers via the Telemetry Gateway.
Uses crosstalk-corrected science images and associated metadata delivered by the Prompt Processing service; uses Master Calibration Images, Template Images, Level 1 Database, and Calibration Database information from the Data Backbone.


.. _mops-payload:

MOPS Payload
------------

Executes under control of the Offline Processing service after a night's observations are complete.
Generates entries in the MOPS Database and the Level 1 Database, including Solar System Object records, measurements, and orbits.
Performs precovery forced photometry of transients.
Uses Level 1 Database entries and images from the Data Backbone.


.. _calibration-qc-payload:

Raw Calibration Validation Payload
----------------------------------

Executes under control of the Prompt Processing service.
Generates raw calibration image quality feedback to the OCS and observers via the Telemetry Gateway.
Uses crosstalk-corrected science images and associated metadata delivered by the Prompt Processing service, Master Calibration Images, and Calibration Database information from the Data Backbone.


.. _daily-cpp-payload:

Daily Calibration Products Update Payload
-----------------------------------------

Executes under control of the OCS-controlled batch processing service so that its execution can be synchronized with the observing schedule.
Uses raw calibration images and information from the Transformed EFD to generate a subset of Master Calibration Images and Calibration Database entries in the Data Backbone.


.. _intermediate-cpp-payload:

Periodic Calibration Products Production Payload
------------------------------------------------

Executes under control of the Template and CPP Execution service at nominally monthly intervals but perhaps as frequently as weekly or as infrequently as quarterly, depending on the stability of Observatory systems and their calibrations.
Uses raw calibration images and information from the Transformed EFD to generate a subset of Master Calibration Images and Calibration Database entries in the Data Backbone.

.. _template-generation-payload:

Template Generation Payload
---------------------------

Executes under control of the Template and CPP Execution service if necessary to generate templates for Alert Production in between annual Data Release Productions.
Uses raw science images to generate the templates, placing them in the Data Backbone.

.. _annual-cpp-payload:

Annual Calibration Products Production Payload
----------------------------------------------

Executes under control of the Template and CPP Execution service at annual intervals prior to the start of the Data Release Production.
Uses raw calibration images, information from the Transformed EFD, information from the Auxiliary Telescope Spectrograph, and external catalogs to generate Master Calibration Images and Calibration Database entries in the Data Backbone.

.. _drp-payload:

Data Release Production Payload
-------------------------------

Executes under control of the DRP Execution service at annual intervals, first running a "mini-DRP" over a small portion of the sky, followed by the full DRP over the entire sky.
Produces science data products in the Data Backbone.


.. _suit:

SUIT
====

The Science User Interface and Tools provide visualization, plotting, catalog rendering, browsing, and searching elements that can be assembled into predetermined "portals" but can also be used flexibly within dynamic "notebook" environments.


.. _middleware:

Middleware
==========

.. _middleware-data-butler:

Data Butler Access Client
-------------------------

The Data Butler provides an access abstraction for all science payloads that enables their underlying data sources and destinations to be configured at runtime with a variety of back-ends ranging from local disk to network locations and a variety of serializations ranging from YAML and FITS files (extensible to HDF5 or ASDF) to database tables.
The Butler client is also available within the LSST Science Platform JupyterLab environment.

.. _middleware-qserv:

Parallel Distributed Database (Qserv)
-------------------------------------

Underlying the catalog data access web service is a parallel distributed database required to handle the petabyte-scale, tens-of-trillions-of-rows catalogs produced by LSST.

.. _middleware-task-framework:

Task Framework
--------------

The Task Framework is a Python class library that provides a structure (standardized class entry points and conventions) to organize low-level algorithms into potentially-reusable algorithmic components (Tasks; e.g. dark frame subtraction, object detection, object measurement), and to organize tasks into basic pipelines (SuperTasks; e.g., process a single visit, build a coadd, difference a visit).
The algorithmic code is written into (Super)Tasks by overriding classes and providing implementation for standard entry points.
The Task Framework allows the pipelines to be constructed and run at the level of a single node or a group of tightly-synchronized nodes.
It allows for sub-node parallelization: trivial parallelization of Task execution, as well as providing (in the future) parallelization primitives for development of multi-core Tasks and synchronized multi-node Tasks.

The Task Framework serves as an interface layer between orchestration and the algorithmic code.
It exposes a standard interface to "activators" (command-line runners as well as the orchestration layer and QA systems), which use it to execute the code wrapped in tasks.
The Task Framework does not concern itself with fault-tolerant massively parallel execution of the pipelines over multiple (thousands) of nodes nor any staging of data that might be required; this is the concern of the orchestration middleware.

The Task Framework exposes to the orchestration system needs and capabilities of the underlying algorithmic code (i.e., the number of cores needed, expected memory-per-core, expected need for data).
It may also receive from the orchestration layer the information on how to optimally run the particular task (i.e., which level of intra-node parallelization is be desired).

It also includes a configuration API and a logging API.


.. _change-record:

#############
Change Record
#############

+-------------+------------+----------------------------------+----------------+
| **Version** | **Date**   | **Description**                  | **Owner**      |
+=============+============+==================================+================+
| 0.1         | 2017-02-17 | Initial draft.                   | Kian-Tat Lim   |
+-------------+------------+----------------------------------+----------------+
| 0.2         | 2017-03-03 | Incorporated feedback.           | Kian-Tat Lim   |
+-------------+------------+----------------------------------+----------------+
| 0.3         | 2017-06-06 | Renamed, reorganized, expanded,  | Kian-Tat Lim   |
|             |            | incorporated feedback.           |                |
+-------------+------------+----------------------------------+----------------+
